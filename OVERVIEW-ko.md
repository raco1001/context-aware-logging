# 맥락 기반 로깅 및 RAG 시스템 (Context-Aware Logging & RAG Observability)

## A. 개발 동기 (Motivation)

### 1. 영감을 주었던 게시글 내용

- [로깅은 엉망이다](https://news.hada.io/topic?id=25239) GeekNews 게시글:
- 현대 분산 시스템에서 많은 경우 개별 서비스는 로그 데이터를 저장하는 방식 또한 개별 적이기에 (monolithic) 다중 서비스 / 캐시 및 데이터 저장소의 맥락을 파악하는게 어렵다는 내용에 주목하게 되었습니다.
- 특히 다음과 같은 부분에 많은 공감을 하게 되었습니다. (게시글의 댓글 내용 포함 요약)

  > 개별 서비스의 로그데이터의 필드 형식은 다양할 수 있어 문자열 검색의 한계가 있다. \
  > 로그 데이터 형식은 주로 쓰기 작업에 주안점을 두어 구성되기에 일관적인 쿼리 방식을 적용하는 것이 힘들다. \
  > 로그는 진실을 말해야 하며, 충분한 맥락과 구조, 그리고 책임(Accountability)을 담고 있어야 한다.

### 2. 게시글에 관심을 가지게 된 배경

- 실무 경험:

  > 서비스 인스턴스 별로 독립된 로그를 남기는 경우,
  > 동일한 요청/이벤트에서 파생된 로그들을 취합하는데 어려움이 많았었고
  > 문제 상황 발생시 즉각 적인 파악에 비효율을 체감했었습니다. \
  > 즉, 맥락(Context)은 서비스, 데이터베이스, 캐시, 큐 사이에 파편화되어 흩어져 있어 데이터 간의 구조나 관계를 추론하는 것에 어려움을 겪었던 경험이 있습니다.

- 프로젝트 경험:

  > 웹 서비스 전반을 구성하는 경험을 하면서
  > 프론트엔드에서의 유저 활동 추적 / 백엔드에서의 요청 수행 로그 / 데이터베이스의 실행 내역을 각각 확인하는 것에 불편함을 느꼈습니다. \
  > 개발 진행 과정 또한 더뎌질 수 밖에 없었고, 이 문제는 개발이 진행될 수록 더 부담이 되었습니다.

- 커뮤니티에서 확인한 공감대
  - MongoDB User Group Korea 세미나에 참석하여 다른 개발자분들로 부터 로깅 전략에 대한 문제에 공감대가 있음을 확인했습니다.
    - 질문 예시 1:

      > '서비스 별 로그 데이터를 분산된 컬렉션에 저장한다고 가정했을 때 어떻게 관리하는게 좋을까요?' \
      >  -> 개별 로그데이터의 저장 컬렉션 또한 나누어져 관리되고 있다면, 각 컬렉션에 일관된 규칙을 적용하고 관리하는 것이 현실적으로 어렵지 않을까 생각했습니다.

    - 질문 예시 2:
      > '현재 시스템에서 MongoDB 컬렉션에 로그데이터를 적재하는데, 조회 작업을 개선하려면 어떻게 해야 할까요?' \
      > -> 당장 떠올랐던 저의 의견은, 시계열 데이터등 로그 데이터라면 반드시 가지고 있을 법한 속성을 기준으로 조회 작업을 최적화 하는 것이었는데,
      > 로그 도큐먼트 별로 저장하는 속성의 종류가 일관적이라고 기대하기는 힘들다는 생각 또한 했습니다.

### 3. 아이디어

- 추후에 MongoDB 커뮤니티에서 이루어진 3 회의 강의 세션 (MongoDB AI Skill Session: Vector Search / RAG / Agentic retrieval)에 참여하고 나서,
  앞서 설명한 Geek News의 내용을 접하게 되었고 다음과 같은 생각을 하게 되었습니다.

  > - Wide Event 형식으로 로그데이터를 남긴다면, 하나의 요청건과 관련한 이벤트, 처리결과 데이터 등으로 로그데이터를 구성했다고 가정.
  > - 이 자체로 일관된 맥락을 가지고 있다고 여겼고, 요청건에 대한 이벤트 및 처리결과 전반을 저장하기에 내용 또한 충분히 풍부할 수 있음.

- 아이디어
  - "하나의 로그 데이터를 1 건의 맥락으로 간주하고 전처리하면 자연어 질의를 통해 쉽게 검색할 수 있다."

- 따라서 두 가지 기능을 우선 구현하는 것을 목표로 프로젝트를 진행했습니다.
  > 1. 하나의 요청건을 처리하는데 발생하는 이벤트 전반을 하나의 로그데이터로 남길 수 있을 것
  > 2. 이 로그데이터를 자연어 질의로 조회할 수 있을 것

### 4. 가정했던 주의점

- AI 기반 시스템(RAG)은 **데이터 유출, 환각(Hallucination), 신뢰 상실**이라는 새로운 리스크를 가져올 수 있다고 생각했습니다.
- 따라서 원본 로그데이터는 하나의 저장소에 순차적으로 저장만 하되, 후에 자연어 질의를 위한 데이터 전처리 작업을 진행하는 것으로 기획했습니다.
- 또한 로그데이터를 전처리하는데에 있어 의미적인 내용의 생성 뿐 만 아니라, LLM 이 특정 형식의 문자열을 리턴할 수 있도록 구성해야 한다고 생각했습니다. 원본의 데이터를 올바른 형식으로 참고할 수 있으면 진실을 전달할 수 있어야 한다는 소기의 목적을 조금이라도 더 만족시킬 수 있기 떄문입니다.

---

## B. 핵심 원칙 (Core Principles)

### 이번 프로젝트에서 구현한 로깅 시스템 및 데이터 저장소는 다음과 같은 원칙 하에 구성했습니다.

- **와이드 이벤트 로깅 (Wide Event Logging)**
  - 하나의 요청(Request) → 하나의 컨텍스트가 풍부한 이벤트로 기록
- **설계 단계부터 고려된 구조화 및 고기수(High-Cardinality) 데이터**
  - 로그는 단순 기록 뿐 만 아니라 '쿼리'또한 고려하여 최적화됨
- **LLM은 '신뢰할 수 없는 엔티티'로 간주**
  - AI 활용 전후로 보안 및 권한 제어 적용
- **설계 기반 보안 및 프라이버시 (Security & Privacy by Design)**
  - 파이프라인의 첫 번째 단계부터 보안을 고려 (컬렉션 분리, 민감데이터는 임베딩 데이터 컬랙션에 포함되지 않도록 함)
- **엔드투엔드 추적성 (End-to-End Traceability)**
  - AI가 생성한 모든 답변은 원본 소스까지 감사가 가능해야 함

---

## C. 프로젝트 로드맵 (Project Phases)

| 단계    | 설명                                                  | 상태 |
| :------ | :---------------------------------------------------- | :--- |
| Phase 1 | NestJS 및 로컬 JSON 로그 기반의 맥락 기반 로깅 구축   | ✅   |
| Phase 2 | MongoDB를 활용한 로깅                                 | ✅   |
| Phase 3 | 요약된 로그 이벤트의 RAG 기반 시맨틱 저장 (Vector DB) | ✅   |
| Phase 4 | RAG 기반의 로그 검색 및 지능형 분석 시스템 구축       | ✅   |
| Phase 5 | 운영 안정화(Hardening): MQ, 캐싱, 샘플링 전략 적용    | ✅   |

---

## D. 아키텍처 철학

### D-1 .백엔드의 코드 구성의 관점

**Phase 1** 부터 Hexagonal Architecture , Layered Architecture 의 철학을 준수하고자 했습니다. 이유는 다음과 같습니다.

- Phase 단계 별로 프로젝트를 구현하는 특징이 있는 만큼, 도메인 영역의 코드는 유지하되, 단계 별로 도메인의 규칙을 동일하게 지키는 서로 다른 외부 서비스 사용 기능(Outbound Adapter)들을 구현하는게 편리하다고 판단했습니다.
- 이번 프로젝트는 로깅 성능의 최적화가 목표가 아니기도 하고, 이해하고 추적하기 쉬운 코드를 유지하는 것이 중요하다고 느꼈습니다.
- 더욱이, 서로를 대체하는 방식으로 구현된 외부 서비스 사용 기능들은 경우에 따라 동일한 런타임에 서로를 보완할 수 있다고도 생각했습니다 \
  ([Phase 6 문서](docs/06-phase-additional.md))

### D-2. 개선 / 전략 추가 (Hardening) 관점

**Phase 5**에서 이 프로젝트는 기능 중심의 RAG 파이프라인을 넘어 **프로덕션 수준의 인프라**에 대해 이해하고자 구성했습니다.
핵심은 자원 관리의 책임 분리(Technical Control)와 회복 탄력성(Resilience)입니다.

#### 1. 수단으로서의 인프라 (Infrastructure as a Means)

- **격리를 위한 Kafka**:
  - Kafka는 로깅 오버헤드를 비즈니스 로직으로부터 **디커플링(Decouple)**하는 버퍼로 사용됩니다.
  - Phase 5에서 로깅시스템이 갑자기 중단되어도 재시작후 이전 로그데이터를 다시 전송할 수 있도록 구현했습니다.
- **상태 관리를 위한 Redis**:
  - 캐싱을 통해 애플리케이션을 무상태(Stateless)로 유지, 수평 확장성을 확보하는 용도로 이해하고,
  - Phase 5에서 로깅 시스템 사용자의 조회 이력을 캐싱하는 용도로 구혔습니다.

#### 2. 트레이드오프

- 과도한 복잡성(Over-engineering)을 피하기 위해 다음 사항들을 의도적으로 선택했습니다:

  **서킷 브레이커**:

  > 로그 재처리의 복잡한 상태 관리 대신, 예측 가능한 서킷 브레이커(Circuit Breaker)를 선택했습니다.

  **해시 기반 샘플링**:

  > 복잡한 동적 정책 대신 해시 기반 샘플링을 사용하여 설명 가능하고 안정적인 로그 볼륨 제어를 구현했습니다.

#### 3. 실험을 통한 검증 (Validation by Experiment)

- 단순 유닛 테스트를 넘어 운영 시나리오(Operational Scenarios)를 통한 시스템 검증을 진행했습니다:

  **Phase 5 개발 완료 후 검수 내용:**

  > 1. 2,000건 단위의 스트레스 테스트를 통해 에러 신호 유실 없이 약 75%의 비용 절감 검증. (로그 종류 별 샘플링 처리)
  > 2. Kafka 장애 시뮬레이션을 통해 DB 직접 저장(Direct-to-DB) fallback 로직이 동작함으로써 데이터 손실이 없음을 확인.
  > 3. 인프라 재시작 시에도 세션 데이터 유지 확인.
  > 4. 인프라 재시작 시에도 MQ 에 머무르던 로그데이터를 백엔드 Consumer 로직이 MongoDB에 정상적으로 저장하는 것을 확인.

---

## E. 작업 기간 및 기술 스택

### 작업 기간:

- 2025.12.24 ~ 2026.01.05 (12일)

### 기술 스택

- **백엔드**: NestJS, TypeScript
- **관측 가능성 구현**: Custom Wide Event Context (AsyncLocalStorage)
- **데이터 저장소**: Local (JSON), MongoDB (Time-series), Vector DB (Pinecone/Atlas)
- **AI / RAG**: LLM (Gemini flash 2.0 / VoyageAI) + Embeddings Module(백엔드)
- **Infra (Local)**: Docker Compose (MongoDB(atlas_local), kafka, zookeeper, redis)
- **Tooling**: Cursor, pnpm, test_data/ 프로그램 (bash, JavaScript)

---

## F. 프로젝트의 지향점 (What This Project Is and Is Not)

✅ **이 프로젝트는:**

- 관측 가능성(Observability), 로깅, 그리고 AI 신뢰성에 대해 탐구했습니다.
- 시스템 디자인 중심의 프로젝트입니다.
- 아키텍처를 고려한 기술적 판단을 남기고자 했습니다.

❌ **이 프로젝트는:**

- 즉시 사용 가능한 SaaS 제품이 아닙니다.
- UI/UX 중심의 애플리케이션이 아닙니다.
- 일반적인 CRUD 데모 프로젝트가 아닙니다.
- 성능을 우선적인 목표로 삼은 프로젝트가 아닙니다.

---

## G. 상세 문서

각 단계별 상세 설계 및 구현 노트는 [docs/](docs/) 디렉토리를 참고해주세요!

---

## H. 면책 조항

- 이 프로젝트는 확장성이나 UI의 완결성보다 **아키텍처, 보안, 그리고 옵저버빌리티의 원칙**을 우선시하여 설계되었습니다.
- 성능 최적화, 로깅 전략 고도화 등의 부가적으로 고려해볼 수 있는 주제는 [docs/의 Phase 6](docs/06-phase-additional.md) 문서에 업데이트 하고 있습니다.

---
