import {
  Injectable,
  Logger,
  OnModuleInit,
  OnModuleDestroy,
} from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { Consumer } from 'kafkajs';
import { MongoLogger, KafkaConsumerClient } from '@logging/infrastructure';
import { WideEvent, LoggingContext } from '@logging/domain';
import { LoggingMode } from '../../core/domain/logging-mode.enum';
import { LoggingModeService } from '../logging-mode.service';

interface LogMessage {
  event: WideEvent;
  _metadata: LoggingContext['_metadata'];
  summary: string;
  timestamp: string;
}

/**
 * MqConsumerService - Background worker that consumes log events from MQ
 * and persists them to MongoDB via MongoLogger.
 *
 * í•µì‹¬ ì² í•™:
 * - ConsumerëŠ” "ephemeral worker"ë¡œ ì·¨ê¸‰ë©ë‹ˆë‹¤.
 * - Kafkaê°€ ì •ìƒì¼ ë•Œë§Œ Consumer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
 * - Kafka ì¥ì•  ì‹œ Consumer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì™„ì „íˆ íŒŒê´´í•©ë‹ˆë‹¤.
 * - Watchdogì€ Consumerë¥¼ ê±´ë“œë¦¬ì§€ ì•Šê³  ë¸Œë¡œì»¤ ê°€ìš©ì„±ë§Œ í™•ì¸í•©ë‹ˆë‹¤.
 *
 * Features:
 * - Batch processing (100 events or 1 second timeout)
 * - Error handling with graceful degradation
 * - State machine-based lifecycle management
 */
@Injectable()
export class MqConsumerService implements OnModuleInit, OnModuleDestroy {
  private readonly logger = new Logger(MqConsumerService.name);
  private consumer: Consumer | null = null; // ğŸ”¥ nullë¡œ ì´ˆê¸°í™”
  private readonly topic: string;
  private readonly batchSize: number;
  private readonly batchTimeoutMs: number;
  private isRunning = false;
  private batch: LogMessage[] = [];
  private batchTimeout: NodeJS.Timeout | null = null;
  private watchdogTimer: NodeJS.Timeout | null = null;
  private consecutiveSuccessCount = 0;
  private readonly STABILITY_THRESHOLD = 3;

  constructor(
    private readonly ConsumerClient: KafkaConsumerClient,
    private readonly mongoLogger: MongoLogger,
    private readonly loggingModeService: LoggingModeService, // ğŸ”¥ ìƒíƒœ ë¨¸ì‹  ì£¼ì…
    private readonly configService: ConfigService,
  ) {
    this.topic = this.configService.get<string>('MQ_LOG_TOPIC') || 'log-events';
    this.batchSize = parseInt(
      this.configService.get<string>('MQ_BATCH_SIZE') || '100',
      10,
    );
    this.batchTimeoutMs = parseInt(
      this.configService.get<string>('MQ_BATCH_TIMEOUT_MS') || '1000',
      10,
    );

    // ğŸ”¥ ìƒíƒœ ë³€ê²½ ê°ì§€ - ëª¨ë“œê°€ ë³€ê²½ë˜ë©´ Consumerë¥¼ ìƒì„±/íŒŒê´´
    this.loggingModeService.onModeChange((mode) => {
      if (mode === LoggingMode.DIRECT) {
        this.logger.log('Mode changed to DIRECT. Destroying consumer...');
        this.destroyConsumer();
      } else if (mode === LoggingMode.KAFKA) {
        this.logger.log('Mode changed to KAFKA. Starting consumer...');
        this.startConsumer();
      }
    });
  }

  async onModuleInit(): Promise<void> {
    // ì´ˆê¸° ëª¨ë“œì— ë”°ë¼ Consumer ì‹œì‘
    if (this.loggingModeService.getMode() === LoggingMode.KAFKA) {
      await this.startConsumer();
    }
  }

  async onModuleDestroy(): Promise<void> {
    await this.destroyConsumer();
    this.stopWatchdog();
  }

  /**
   * Consumerë¥¼ ìƒì„±í•˜ê³  ì‹œì‘í•©ë‹ˆë‹¤.
   * Kafkaê°€ ì •ìƒì¼ ë•Œë§Œ í˜¸ì¶œë©ë‹ˆë‹¤.
   */
  private async startConsumer(): Promise<void> {
    if (this.consumer) {
      this.logger.debug('Consumer already exists, skipping...');
      return;
    }

    try {
      // ğŸ”¥ Consumer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
      this.consumer = await this.ConsumerClient.createAndConnect();

      await this.consumer.subscribe({
        topic: this.topic,
        fromBeginning: false,
      });

      this.isRunning = true;
      this.stopWatchdog();

      this.logger.log(
        `Started MQ consumer for topic: ${this.topic}, group: ${this.ConsumerClient.getGroupId()}`,
      );

      // ğŸ”¥ consume() ì‹¤í–‰
      this.consume().catch((error) => {
        this.logger.error(
          `Consumer runtime error: ${error.message}`,
          error.stack,
        );
        this.handleConsumerFailure();
      });
    } catch (error) {
      this.logger.error(
        `Failed to start consumer: ${error.message}`,
        error.stack,
      );
      this.handleConsumerFailure();
    }
  }

  /**
   * Consumerë¥¼ ì™„ì „íˆ íŒŒê´´í•©ë‹ˆë‹¤.
   * disconnect() í›„ ë°˜ë“œì‹œ nullë¡œ ì„¤ì •í•˜ì—¬ GC ëŒ€ìƒìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
   */
  private async destroyConsumer(): Promise<void> {
    if (!this.consumer) {
      return;
    }

    this.isRunning = false;

    try {
      // ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ì´ë©´ ì™„ë£Œ ëŒ€ê¸°
      if (this.batch.length > 0) {
        this.logger.log(
          `Processing final batch of ${this.batch.length} events before destroying consumer...`,
        );
        await this.flushBatch();
      }

      // Consumer ì¤‘ì§€
      if (this.consumer) {
        await this.consumer.stop();
      }
    } catch (error) {
      this.logger.warn(`Error stopping consumer: ${error.message}`);
    }

    // ğŸ”¥ ConsumerClientë¥¼ í†µí•´ ì™„ì „ íŒŒê´´
    await this.ConsumerClient.destroy();
    this.consumer = null; // ğŸ”¥ nullë¡œ ì„¤ì •

    // ë°°ì¹˜ íƒ€ì´ë¨¸ ì •ë¦¬
    if (this.batchTimeout) {
      clearTimeout(this.batchTimeout);
      this.batchTimeout = null;
    }

    this.logger.log('Consumer destroyed');
  }

  /**
   * Consumer ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
   * ìƒíƒœë¥¼ DIRECTë¡œ ë³€ê²½í•˜ì—¬ Consumer íŒŒê´´ë¥¼ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.
   */
  private handleConsumerFailure(): void {
    this.logger.warn(
      'Consumer failure detected. Switching to DIRECT mode and starting watchdog.',
    );

    // ìƒíƒœë¥¼ DIRECTë¡œ ë³€ê²½ (ì´ê²ƒì´ Consumer íŒŒê´´ë¥¼ íŠ¸ë¦¬ê±°í•¨)
    this.loggingModeService.setMode(LoggingMode.DIRECT);

    // Watchdog ì‹œì‘
    this.startWatchdog();
  }

  /**
   * Watchdog: Kafka ë¸Œë¡œì»¤ ê°€ìš©ì„±ë§Œ í™•ì¸
   * Consumerë¥¼ ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
   */
  private startWatchdog(): void {
    if (this.watchdogTimer) {
      return;
    }

    this.consecutiveSuccessCount = 0;
    this.logger.log(
      'Watchdog started. Monitoring Kafka broker availability...',
    );

    this.watchdogTimer = setInterval(async () => {
      try {
        const isAvailable = await this.ConsumerClient.checkBrokerAvailability();

        if (isAvailable) {
          this.consecutiveSuccessCount++;
          this.logger.debug(
            `Watchdog: Kafka available (${this.consecutiveSuccessCount}/${this.STABILITY_THRESHOLD})`,
          );

          if (this.consecutiveSuccessCount >= this.STABILITY_THRESHOLD) {
            this.logger.log(
              'Watchdog: Kafka is stable. Switching to KAFKA mode...',
            );
            this.consecutiveSuccessCount = 0;
            this.stopWatchdog();

            // ğŸ”¥ ìƒíƒœ ë³€ê²½ë§Œ í•˜ë©´ ë¨ - onModeChange ì½œë°±ì´ Consumerë¥¼ ìƒì„±í•¨
            this.loggingModeService.setMode(LoggingMode.KAFKA);
          }
        } else {
          this.consecutiveSuccessCount = 0;
          this.logger.debug('Watchdog: Kafka still offline.');
        }
      } catch (error) {
        this.consecutiveSuccessCount = 0;
        this.logger.debug(`Watchdog error: ${error.message}`);
      }
    }, 60000); // 1ë¶„ë§ˆë‹¤ ì²´í¬
  }

  private stopWatchdog(): void {
    if (this.watchdogTimer) {
      clearInterval(this.watchdogTimer);
      this.watchdogTimer = null;
    }
  }

  private async consume(): Promise<void> {
    if (!this.consumer) {
      throw new Error('Consumer instance does not exist');
    }

    await this.consumer.run({
      eachMessage: async ({ topic, partition, message }) => {
        try {
          if (!message.value) {
            this.logger.warn('Received message with no value');
            return;
          }

          const logMessage: LogMessage = JSON.parse(message.value.toString());
          this.batch.push(logMessage);

          // Process batch if it reaches the size limit
          if (this.batch.length >= this.batchSize) {
            await this.flushBatch();
          } else {
            // Set timeout for batch processing
            this.scheduleBatchFlush();
          }
        } catch (error) {
          this.logger.error(
            `Error processing message from topic ${topic}, partition ${partition}: ${error.message}`,
            error.stack,
          );
        }
      },
      // Disable KafkaJS auto-restart - we handle recovery via state machine
      restartOnFailure: async (error) => {
        this.logger.warn(
          `Consumer error: ${error.message}. Disabling KafkaJS auto-restart.`,
        );
        // ğŸ”¥ false ë°˜í™˜í•˜ì—¬ KafkaJS ìë™ ì¬ì‹œì‘ ë¹„í™œì„±í™”
        // ìƒíƒœ ë¨¸ì‹ ì´ Consumer íŒŒê´´ ë° ë³µêµ¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
        return false;
      },
    } as any); // Type assertion: restartOnFailure is supported in KafkaJS but may not be in TypeScript types yet
  }

  private scheduleBatchFlush(): void {
    if (this.batchTimeout) {
      return;
    }

    this.batchTimeout = setTimeout(async () => {
      this.batchTimeout = null;
      if (this.batch.length > 0) {
        await this.flushBatch();
      }
    }, this.batchTimeoutMs);
  }

  private async flushBatch(): Promise<void> {
    if (this.batch.length === 0) {
      return;
    }

    if (this.batchTimeout) {
      clearTimeout(this.batchTimeout);
      this.batchTimeout = null;
    }

    const batchToProcess = [...this.batch];
    this.batch = [];

    await this.processBatch(batchToProcess);
  }

  private async processBatch(batch: LogMessage[]): Promise<void> {
    const startTime = Date.now();
    let successCount = 0;
    let failureCount = 0;

    for (const message of batch) {
      try {
        await this.mongoLogger.log(
          message.event,
          message._metadata,
          message.summary,
        );
        successCount++;
      } catch (error) {
        failureCount++;
        this.logger.error(
          `Failed to persist log event (requestId: ${message.event.requestId}): ${error.message}`,
          error.stack,
        );
      }
    }

    const duration = Date.now() - startTime;
    this.logger.log(
      `Processed batch: ${batch.length} events (${successCount} success, ${failureCount} failures) in ${duration}ms`,
    );
  }
}
